{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 2\n",
    "## Author: Haonan Tian "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import time\n",
    "import os\n",
    "import io\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from collections import Counter\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_path = os.getcwd()\n",
    "def read_data(path, filename):\n",
    "    with open(current_path+'/hw2_data/' + filename) as tsv:\n",
    "        reader = csv.reader(tsv, delimiter='\\t')\n",
    "        data_pre = []\n",
    "        data_post = []\n",
    "        label = []\n",
    "        for row in reader:\n",
    "            data_pre.append(row[0])\n",
    "            data_post.append(row[1])\n",
    "            label.append(row[2])\n",
    "        return data_pre[1:], data_post[1:], label[1:] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data_mnli(path, filename):\n",
    "    with open(current_path+'/hw2_data/' + filename) as tsv:\n",
    "        reader = csv.reader(tsv, delimiter='\\t')\n",
    "        counter = 0\n",
    "        genre = []\n",
    "        result = {}\n",
    "        for row in reader:\n",
    "            temp_list = []\n",
    "            if counter > 0:\n",
    "                genre = row[3]\n",
    "                if genre in result:\n",
    "                    temp_list.append(row[0])\n",
    "                    temp_list.append(row[1])\n",
    "                    temp_list.append(row[2])\n",
    "                    result[genre].append(temp_list)\n",
    "                else:\n",
    "                    result[genre] = []\n",
    "                    temp_list.append(row[0])\n",
    "                    temp_list.append(row[1])\n",
    "                    temp_list.append(row[2])\n",
    "                    result[genre].append(temp_list)\n",
    "            else:\n",
    "                counter += 1\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "snli_train_pre, snli_train_post, snli_train_label = read_data(current_path, 'snli_train.tsv') \n",
    "snli_val_pre, snli_val_post, snli_val_label = read_data(current_path, 'snli_val.tsv') \n",
    "mnli_train = read_data_mnli(current_path, 'mnli_train.tsv') \n",
    "mnli_val= read_data_mnli(current_path, 'mnli_val.tsv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['fiction', 'telephone', 'slate', 'government', 'travel'])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnli_val.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load embedding layers\n",
    "def load_vectors(fname, vocab_size):\n",
    "    fin = io.open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "    n, d = map(int, fin.readline().split())\n",
    "    data = {}\n",
    "    counter = 0\n",
    "    for line in fin:\n",
    "        tokens = line.rstrip().split(' ')\n",
    "        temp_list = []\n",
    "        for item in tokens[1:]:\n",
    "            temp_list.append(float(item))\n",
    "        data[tokens[0]] = temp_list\n",
    "        if counter % 1000 == 0:\n",
    "            print('Finished line {}'.format(counter))\n",
    "        counter += 1\n",
    "        if counter >= vocab_size:\n",
    "            break;\n",
    "    return d, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished line 0\n",
      "Finished line 1000\n",
      "Finished line 2000\n",
      "Finished line 3000\n",
      "Finished line 4000\n",
      "Finished line 5000\n",
      "Finished line 6000\n",
      "Finished line 7000\n",
      "Finished line 8000\n",
      "Finished line 9000\n",
      "Finished line 10000\n",
      "Finished line 11000\n",
      "Finished line 12000\n",
      "Finished line 13000\n",
      "Finished line 14000\n",
      "Finished line 15000\n",
      "Finished line 16000\n",
      "Finished line 17000\n",
      "Finished line 18000\n",
      "Finished line 19000\n",
      "Finished line 20000\n",
      "Finished line 21000\n",
      "Finished line 22000\n",
      "Finished line 23000\n",
      "Finished line 24000\n",
      "Finished line 25000\n",
      "Finished line 26000\n",
      "Finished line 27000\n",
      "Finished line 28000\n",
      "Finished line 29000\n",
      "Finished line 30000\n",
      "Finished line 31000\n",
      "Finished line 32000\n",
      "Finished line 33000\n",
      "Finished line 34000\n",
      "Finished line 35000\n",
      "Finished line 36000\n",
      "Finished line 37000\n",
      "Finished line 38000\n",
      "Finished line 39000\n",
      "Finished line 40000\n",
      "Finished line 41000\n",
      "Finished line 42000\n",
      "Finished line 43000\n",
      "Finished line 44000\n",
      "Finished line 45000\n",
      "Finished line 46000\n",
      "Finished line 47000\n",
      "Finished line 48000\n",
      "Finished line 49000\n"
     ]
    }
   ],
   "source": [
    "embed_dim, embed_dist = load_vectors(current_path+'/hw2_data/wiki-news-300d-1M.vec', 50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create label set\n",
    "def convert_label(label_list):\n",
    "    result = []\n",
    "    for item in label_list:\n",
    "        if item == 'contradiction':\n",
    "            result.append(0)\n",
    "        elif item == 'entailment':\n",
    "            result.append(1)\n",
    "        elif item == 'neutral':\n",
    "            result.append(2)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "snli_train_label = convert_label(snli_train_label)\n",
    "snli_val_label = convert_label(snli_val_label)\n",
    "mnli_train_label = convert_label(mnli_train_label)\n",
    "mnli_val_label = convert_label(mnli_val_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_token(input_set):\n",
    "    result = []\n",
    "    counter = 0\n",
    "    for row in input_set:\n",
    "        result.append(row.split(' '))\n",
    "        if counter % 5000 == 0:\n",
    "            print('FINISHED {}'.format(counter))\n",
    "        counter += 1\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINISHED 0\n",
      "FINISHED 5000\n",
      "FINISHED 10000\n",
      "FINISHED 15000\n",
      "FINISHED 20000\n",
      "FINISHED 25000\n",
      "FINISHED 30000\n",
      "FINISHED 35000\n",
      "FINISHED 40000\n",
      "FINISHED 45000\n",
      "FINISHED 50000\n",
      "FINISHED 55000\n",
      "FINISHED 60000\n",
      "FINISHED 65000\n",
      "FINISHED 70000\n",
      "FINISHED 75000\n",
      "FINISHED 80000\n",
      "FINISHED 85000\n",
      "FINISHED 90000\n",
      "FINISHED 95000\n",
      "FINISHED 0\n",
      "FINISHED 5000\n",
      "FINISHED 10000\n",
      "FINISHED 15000\n",
      "FINISHED 20000\n",
      "FINISHED 25000\n",
      "FINISHED 30000\n",
      "FINISHED 35000\n",
      "FINISHED 40000\n",
      "FINISHED 45000\n",
      "FINISHED 50000\n",
      "FINISHED 55000\n",
      "FINISHED 60000\n",
      "FINISHED 65000\n",
      "FINISHED 70000\n",
      "FINISHED 75000\n",
      "FINISHED 80000\n",
      "FINISHED 85000\n",
      "FINISHED 90000\n",
      "FINISHED 95000\n",
      "FINISHED 0\n",
      "FINISHED 0\n",
      "FINISHED 0\n",
      "FINISHED 5000\n",
      "FINISHED 10000\n",
      "FINISHED 15000\n",
      "FINISHED 0\n",
      "FINISHED 5000\n",
      "FINISHED 10000\n",
      "FINISHED 15000\n",
      "FINISHED 0\n",
      "FINISHED 0\n"
     ]
    }
   ],
   "source": [
    "snli_train_pre = convert_to_token(snli_train_pre)\n",
    "snli_train_post = convert_to_token(snli_train_post)\n",
    "\n",
    "snli_val_pre = convert_to_token(snli_val_pre) \n",
    "snli_val_post = convert_to_token(snli_val_post)\n",
    "\n",
    "mnli_train_pre = convert_to_token(mnli_train_pre)\n",
    "mnli_train_post = convert_to_token(mnli_train_post) \n",
    "\n",
    "mnli_val_pre = convert_to_token(mnli_val_pre) \n",
    "mnli_val_post = convert_to_token(mnli_val_post) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all_tokens(input_set):\n",
    "    all_sentence = []\n",
    "    for sentence in input_set:\n",
    "        all_sentence += sentence\n",
    "    output = Counter(all_sentence)\n",
    "    result = []\n",
    "    for key, value in output.items():\n",
    "        result.append(key)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens = find_all_tokens(snli_train_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lookup(all_tokens, embed_dist):\n",
    "    result = {}\n",
    "    counter = 1\n",
    "    for item in all_tokens:\n",
    "        if item in embed_dist:\n",
    "            result[item] = counter\n",
    "        #else:\n",
    "        #    result[item] = 0\n",
    "        counter += 1\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup = build_lookup(all_tokens, embed_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rebuild_embed_dict(embed_dict, lookup, embed_dim):\n",
    "    result = {}\n",
    "    result[0] = [0 for i in range(embed_dim)]\n",
    "    for key, value in lookup.items():\n",
    "        result[value] = embed_dict[key]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_embed_dict = rebuild_embed_dict(embed_dist, lookup, embed_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_idx(input_set, lookup):\n",
    "    result = []\n",
    "    for sentence in input_set:\n",
    "        temp_result = []\n",
    "        for token in sentence:\n",
    "            if token in lookup:\n",
    "                temp_result.append(lookup[token])\n",
    "        result.append(temp_result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "snli_train_pre = convert_to_idx(snli_train_pre, lookup)\n",
    "snli_train_post = convert_to_idx(snli_train_post, lookup)\n",
    "\n",
    "snli_val_pre = convert_to_idx(snli_val_pre, lookup)\n",
    "snli_val_post = convert_to_idx(snli_val_post, lookup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_max_length(list1, list2):\n",
    "    length1 = []\n",
    "    length2 = []\n",
    "    for item in list1:\n",
    "        length1.append(len(item))\n",
    "    for item in list2:\n",
    "        length2.append(len(item))\n",
    "    return max([max(length1), max(length2)]), length1, length2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_WORD_LENGTH, snli_train_pre_len, snli_train_post_len = find_max_length(snli_train_pre, snli_train_post)\n",
    "_, snli_val_pre_len, snli_val_post_len = find_max_length(snli_val_pre, snli_val_post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_each_genre(data_set, embed_dist, embed_dim):\n",
    "    result = {}\n",
    "    for genre, value in data_set.items():\n",
    "        temp_list = []\n",
    "        pre = []\n",
    "        post = []\n",
    "        label = []\n",
    "        for sample in data_set:\n",
    "            pre.append(sample[0])\n",
    "            post.append(sample[1])\n",
    "            label.append(sample[2])\n",
    "        all_tokens = find_all_tokens(pre+post)\n",
    "        lookup = build_lookup(all_tokens, embed_dist)\n",
    "        idx_embed_dict = rebuild_embed_dict(embed_dist, lookup, embed_dim)\n",
    "        pre = convert_to_idx(pre)\n",
    "        post = convert_to_idx(post)\n",
    "        max_length, pre_lengths, post_lengths = find_max_length(pre, post)\n",
    "        secure_idx(pre, max_length)\n",
    "        secure_idx(post, max_length)\n",
    "        temp_list.append(pre)\n",
    "        temp_list.append(post)\n",
    "        temp_list.append(pre_lengths)\n",
    "        temp_list.append(post_lengths)\n",
    "        temp_list.append(convert_label(label))\n",
    "        result[genre] = temp_list\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnli_train = setup_each_genre(mnli_train)\n",
    "mnli_val = setup_each_genre(mnli_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class localDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_pre, data_post, pre_length, post_length, data_label):\n",
    "        \"\"\"\n",
    "        @param data_list: list of character\n",
    "        @param target_list: list of targets\n",
    "\n",
    "        \"\"\"\n",
    "        self.data_pre, self.data_post, self.data_label = data_pre, data_post, data_label\n",
    "        self.pre_length = pre_length\n",
    "        self.post_length = post_length\n",
    "        assert (len(self.data_pre) == len(self.data_post)) and (len(self.data_pre) == len(self.data_label))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_pre)\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        \"\"\"\n",
    "        idx_pre = self.data_pre[key]\n",
    "        idx_post = self.data_post[key]\n",
    "        pre_length = self.pre_length[key]\n",
    "        post_length = self.post_length[key]\n",
    "        idx_label = self.data_label[key]\n",
    "        return [idx_pre, idx_post, pre_length, post_length, idx_label]\n",
    "\n",
    "def vocab_collate_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all\n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    data_pre = []\n",
    "    data_post = []\n",
    "    label_list = []\n",
    "    length_pre = []\n",
    "    length_post = []\n",
    "\n",
    "    for datum in batch:\n",
    "        label_list.append(datum[4])\n",
    "        length_pre.append(datum[2])\n",
    "        length_post.append(datum[3])\n",
    "    # padding\n",
    "    for datum in batch:\n",
    "        padded_vec_pre = np.pad(np.array(datum[0]),\n",
    "                                pad_width=((0,MAX_WORD_LENGTH-datum[2])),\n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        data_pre.append(padded_vec_pre)\n",
    "        padded_vec_post = np.pad(np.array(datum[1]),\n",
    "                                pad_width=((0,MAX_WORD_LENGTH-datum[3])),\n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        data_post.append(padded_vec_post)\n",
    "    \n",
    "    pre_dec_order = np.argsort(length_pre)[::-1] # Sort all list accoridng to length in decreasing order\n",
    "    post_dec_order = np.argsort(length_post)[::-1]\n",
    "    data_pre = np.array(data_pre)[pre_dec_order]\n",
    "    data_post = np.array(data_post)[pre_dec_order]\n",
    "    length_pre = np.array(length_pre)[pre_dec_order]\n",
    "    length_post = np.array(length_post)[post_dec_order]\n",
    "    label_list = np.array(label_list)[pre_dec_order]\n",
    "    data_pre = torch.from_numpy(np.array(data_pre))\n",
    "    data_pre = data_pre.type('torch.FloatTensor')\n",
    "    data_post = torch.from_numpy(np.array(data_post))\n",
    "    data_post = data_post.type('torch.FloatTensor')\n",
    "    return [data_pre, data_post, torch.LongTensor(length_pre), torch.LongTensor(length_post), torch.LongTensor(label_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build train, valid and test dataloaders\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "snli_train_dataset = localDataset(snli_train_pre, snli_train_post, snli_train_pre_len, snli_train_post_len, snli_train_label)\n",
    "snli_train_loader = torch.utils.data.DataLoader(dataset=snli_train_dataset,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=vocab_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "snli_val_dataset = localDataset(snli_val_pre, snli_val_post, snli_val_pre_len, snli_val_post_len, snli_val_label)\n",
    "snli_val_loader = torch.utils.data.DataLoader(dataset=snli_val_dataset,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=vocab_collate_func,\n",
    "                                           shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU(nn.Module):\n",
    "    def __init__(self, emb_size, hidden_size, num_layers, num_classes):\n",
    "        # RNN Accepts the following hyperparams:\n",
    "        # emb_size: Embedding Size\n",
    "        # hidden_size: Hidden Size of layer in RNN\n",
    "        # num_layers: number of layers in RNN\n",
    "        # num_classes: number of output classes\n",
    "        # vocab_size: vocabulary size\n",
    "        super(GRU, self).__init__()\n",
    "\n",
    "        self.num_layers, self.hidden_size = num_layers, hidden_size\n",
    "        #self.embedding = nn.Embedding(vocab_size, emb_size, padding_idx=PAD_IDX)\n",
    "        self.rnn = nn.GRU(emb_size, hidden_size, num_layers, batch_first=True, bidirectional=True)\n",
    "        # Details can be seen at https://pytorch.org/docs/stable/_modules/torch/nn/modules/rnn.html \n",
    "        self.linear = nn.Linear(4*hidden_size, num_classes)\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        # Function initializes the activation of recurrent neural net at timestep 0\n",
    "        # Needs to be in format (num_layers, batch_size, hidden_size)\n",
    "        hidden = torch.randn(2*self.num_layers, batch_size, self.hidden_size)\n",
    "\n",
    "        return hidden\n",
    "    \n",
    "    def perform_embed(self, data_set, idx_embed_dict):\n",
    "        result = []\n",
    "        for sentence in data_set.numpy().tolist():\n",
    "            temp_result = []\n",
    "            for idx in sentence:\n",
    "                temp_result.append(idx_embed_dict[idx])\n",
    "            result.append(temp_result)\n",
    "        return torch.from_numpy(np.array(result))\n",
    "\n",
    "    def forward(self, data_pre, data_post, pre_lengths, post_lengths, idx_embed_dict):\n",
    "        # reset hidden state\n",
    "\n",
    "        batch_size,_ = data_pre.size()\n",
    "\n",
    "        self.hidden_pre = self.init_hidden(batch_size)\n",
    "        self.hidden_post = self.init_hidden(batch_size)\n",
    "        \n",
    "\n",
    "        # get embedding of characters\n",
    "        embed_pre = self.perform_embed(data_pre, idx_embed_dict).type('torch.FloatTensor')\n",
    "        embed_post = self.perform_embed(data_post, idx_embed_dict).type('torch.FloatTensor')\n",
    "        \n",
    "        # pack padded sequence\n",
    "        embed_pre = torch.nn.utils.rnn.pack_padded_sequence(embed_pre, pre_lengths.numpy(), batch_first=True)\n",
    "        embed_post = torch.nn.utils.rnn.pack_padded_sequence(embed_post, post_lengths.numpy(), batch_first=True)\n",
    "        # According to Pytorch, the pack_padded_sequance take input with dimension batch_size if batch_first == True\n",
    "        # and maximum sequence length and embedding dimensions refer to https://pytorch.org/docs/stable/nn.html \n",
    "        \n",
    "        # fprop though RNN\n",
    "        print(self.hidden_pre.size())\n",
    "        rnn_out_pre, self.hidden_pre = self.rnn(embed_pre, self.hidden_pre)\n",
    "        rnn_out_post, self.hidden_post = self.rnn(embed_post, self.hidden_post)\n",
    "        \n",
    "        # undo packing\n",
    "        rnn_out_pre, _ = torch.nn.utils.rnn.pad_packed_sequence(rnn_out_pre, batch_first=True)\n",
    "        rnn_out_post, _ = torch.nn.utils.rnn.pad_packed_sequence(rnn_out_post, batch_first=True)\n",
    "        \n",
    "        # sum hidden activations of RNN across time\n",
    "        rnn_out_pre = torch.sum(rnn_out_pre, dim=1)\n",
    "        rnn_out_post = torch.sum(rnn_out_post, dim=1)\n",
    "        rnn_out = torch.cat((rnn_out_pre, rnn_out_post), 1)\n",
    "\n",
    "        logits = self.linear(rnn_out)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 64, 200])\n",
      "torch.Size([4, 64, 200])\n",
      "torch.Size([4, 64, 200])\n",
      "torch.Size([4, 64, 200])\n",
      "torch.Size([4, 64, 200])\n",
      "torch.Size([4, 64, 200])\n",
      "torch.Size([4, 64, 200])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-09b2b123c296>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;31m# Backward and optimize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;31m# validate every 100 iterations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlpclass/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlpclass/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def test_model(loader, model, idx_embed_dict):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    for data_pre, data_post, lengths_pre, lengths_post, labels in loader:\n",
    "        data_batch_pre, lengths_batch_pre, data_batch_post, lengths_batch_post, label_batch = data_pre, lengths_pre, data_post, lengths_post, labels\n",
    "        outputs = F.softmax(model(data_batch_pre, data_batch_post, lengths_batch_pre, lengths_batch_post, idx_embed_dict), dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels.view_as(predicted)).sum().item()\n",
    "    return (100 * correct / total)\n",
    "\n",
    "\n",
    "model = GRU(emb_size=embed_dim, hidden_size=200, num_layers=2, num_classes=3)\n",
    "\n",
    "learning_rate = 3e-4\n",
    "num_epochs = 5 # number epoch to train\n",
    "\n",
    "# Criterion and Optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "total_step = len(snli_val_loader)\n",
    "#print(len(snli_train_loader))\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (data_pre, data_post, pre_lengths, post_lengths, labels) in enumerate(snli_val_loader):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        outputs = model(data_pre, data_post, pre_lengths, post_lengths, idx_embed_dict)\n",
    "        #print('Output {} Label {}'.format(outputs.size(), labels.size()))\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # validate every 100 iterations\n",
    "        if i > 0 and i % 10 == 0:\n",
    "            # validate\n",
    "            val_acc = test_model(snli_val_loader, model, idx_embed_dict)\n",
    "            print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}'.format(\n",
    "                       epoch+1, num_epochs, i+1, len(snli_train_loader), val_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class localDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_pre, data_post, pre_length, post_length, data_label):\n",
    "        \"\"\"\n",
    "        @param data_list: list of character\n",
    "        @param target_list: list of targets\n",
    "\n",
    "        \"\"\"\n",
    "        self.data_pre, self.data_post, self.data_label = data_pre, data_post, data_label\n",
    "        self.pre_length = pre_length\n",
    "        self.post_length = post_length\n",
    "        assert (len(self.data_pre) == len(self.data_post)) and (len(self.data_pre) == len(self.data_label))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_pre)\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        \"\"\"\n",
    "        idx_pre = self.data_pre[key]\n",
    "        idx_post = self.data_post[key]\n",
    "        pre_length = self.pre_length[key]\n",
    "        post_length = self.post_length[key]\n",
    "        idx_label = self.data_label[key]\n",
    "        return [idx_pre, idx_post, pre_length, post_length, idx_label]\n",
    "\n",
    "def vocab_collate_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all\n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    data_pre = []\n",
    "    data_post = []\n",
    "    label_list = []\n",
    "    length_pre = []\n",
    "    length_post = []\n",
    "\n",
    "    for datum in batch:\n",
    "        label_list.append(datum[4])\n",
    "        length_pre.append(datum[2])\n",
    "        length_post.append(datum[3])\n",
    "    # padding\n",
    "    for datum in batch:\n",
    "        padded_vec_pre = np.pad(np.array(datum[0]),\n",
    "                                pad_width=((0,MAX_WORD_LENGTH-datum[2])),\n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        data_pre.append(padded_vec_pre)\n",
    "        padded_vec_post = np.pad(np.array(datum[1]),\n",
    "                                pad_width=((0,MAX_WORD_LENGTH-datum[3])),\n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        data_post.append(padded_vec_post)\n",
    "    \n",
    "    pre_dec_order = np.argsort(length_pre)[::-1] # Sort all list accoridng to length in decreasing order\n",
    "    post_dec_order = np.argsort(length_post)[::-1]\n",
    "    data_pre = np.array(data_pre)[pre_dec_order]\n",
    "    data_post = np.array(data_post)[pre_dec_order]\n",
    "    length_pre = np.array(length_pre)[pre_dec_order]\n",
    "    length_post = np.array(length_post)[post_dec_order]\n",
    "    label_list = np.array(label_list)[pre_dec_order]\n",
    "    data_pre = torch.from_numpy(np.array(data_pre)).type('torch.DoubleTensor')\n",
    "    data_post = torch.from_numpy(np.array(data_post)).type('torch.DoubleTensor')\n",
    "    return [data_pre, data_post, torch.LongTensor(length_pre), torch.LongTensor(length_post), torch.LongTensor(label_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build train, valid and test dataloaders\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "snli_train_dataset = localDataset(snli_train_pre, snli_train_post, snli_train_pre_len, snli_train_post_len, snli_train_label)\n",
    "snli_train_loader = torch.utils.data.DataLoader(dataset=snli_train_dataset,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=vocab_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "snli_val_dataset = localDataset(snli_val_pre, snli_val_post, snli_val_pre_len, snli_val_post_len, snli_val_label)\n",
    "snli_val_loader = torch.utils.data.DataLoader(dataset=snli_val_dataset,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=vocab_collate_func,\n",
    "                                           shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, emb_size, hidden_size, num_layers, num_classes, max_length):\n",
    "\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        self.num_layers, self.hidden_size = num_layers, hidden_size\n",
    "    \n",
    "        self.conv1 = nn.Conv1d(emb_size, hidden_size, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(hidden_size, hidden_size, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.maxpool = nn.MaxPool1d(max_length, stride=1)\n",
    "\n",
    "        self.linear = nn.Linear(2*hidden_size, num_classes)\n",
    "    \n",
    "    def perform_embed(self, data_set, idx_embed_dict):\n",
    "        result = []\n",
    "        for sentence in data_set.numpy().tolist():\n",
    "            temp_result = []\n",
    "            for idx in sentence:\n",
    "                temp_result.append(idx_embed_dict[idx])\n",
    "            result.append(temp_result)\n",
    "        return torch.from_numpy(np.array(result))\n",
    "\n",
    "    def forward(self, data_pre, data_post, idx_embed_dict):\n",
    "        batch_size, max_lengths = data_pre.size()\n",
    "\n",
    "        data_pre = self.perform_embed(data_pre, idx_embed_dict).type('torch.FloatTensor')\n",
    "        data_post = self.perform_embed(data_post, idx_embed_dict).type('torch.FloatTensor')\n",
    "        \n",
    "        hidden_pre = self.conv1(data_pre.transpose(1,2)).transpose(1,2)\n",
    "        hidden_pre = F.relu(hidden_pre.contiguous().view(-1, hidden_pre.size(-1))).view(batch_size, max_lengths, hidden_pre.size(-1))\n",
    "        \n",
    "        hidden_pre = self.conv2(hidden_pre.transpose(1,2)).transpose(1,2)\n",
    "        hidden_pre = F.relu(hidden_pre.contiguous().view(-1, hidden_pre.size(-1))).view(batch_size, max_lengths, hidden_pre.size(-1))\n",
    "        \n",
    "        hidden_post = self.conv1(data_post.transpose(1,2)).transpose(1,2)\n",
    "        hidden_post = F.relu(hidden_post.contiguous().view(-1, hidden_post.size(-1))).view(batch_size, max_lengths, hidden_post.size(-1))\n",
    "        \n",
    "        hidden_post = self.conv2(hidden_post.transpose(1,2)).transpose(1,2)\n",
    "        hidden_post = F.relu(hidden_post.contiguous().view(-1, hidden_post.size(-1))).view(batch_size, max_lengths, hidden_post.size(-1))\n",
    "        \n",
    "        hidden_pre = hidden_pre.transpose(1,2)\n",
    "        hidden_pre = self.maxpool(hidden_pre)\n",
    "        hidden_pre = hidden_pre.transpose(1,2).squeeze(1)\n",
    "        \n",
    "        hidden_post = hidden_post.transpose(1,2)\n",
    "        hidden_post = self.maxpool(hidden_post)\n",
    "        hidden_post = hidden_post.transpose(1,2).squeeze(1)\n",
    "        \n",
    "        hidden = torch.cat((hidden_pre, hidden_post), 1)\n",
    "        logits = self.linear(hidden)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(loader, model, idx_embed_dict):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    for data_pre, data_post, pre_lengths, post_lengths, labels in loader:\n",
    "        data_batch_pre, data_batch_post, lengths_batch_pre, lengths_batch_post, label_batch = data_pre, data_post, pre_lengths, post_lengths, labels\n",
    "        outputs = F.softmax(model(data_batch_pre, data_batch_post, idx_embed_dict), dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels.view_as(predicted)).sum().item()\n",
    "    return (100 * correct / total)\n",
    "\n",
    "\n",
    "model = CNN(emb_size=300, hidden_size=200, num_layers=2, num_classes=3, max_length=MAX_WORD_LENGTH)\n",
    "\n",
    "learning_rate = 3e-4\n",
    "num_epochs = 5 # number epoch to train\n",
    "\n",
    "# Criterion and Optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "total_step = len(snli_val_loader)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (data_pre, data_post, pre_lengths, post_lengths, labels) in enumerate(snli_train_loader):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        outputs = model(data_pre, data_post, idx_embed_dict)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # validate every 100 iterations\n",
    "        if i > 0 and i % 10 == 0:\n",
    "            # validate\n",
    "            val_acc = test_model(snli_val_loader, model, idx_embed_dict)\n",
    "            print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}'.format(\n",
    "                       epoch+1, num_epochs, i+1, len(snli_train_loader), val_acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
