{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 2\n",
    "## Author: Haonan Tian "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import time\n",
    "import os\n",
    "import io\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_path = os.getcwd()\n",
    "def read_data(path, filename):\n",
    "    with open(current_path+'/hw2_data/' + filename) as tsv:\n",
    "        reader = csv.reader(tsv, delimiter='\\t')\n",
    "        data_pre = []\n",
    "        data_post = []\n",
    "        label = []\n",
    "        for row in reader:\n",
    "            data_pre.append(row[0])\n",
    "            data_post.append(row[1])\n",
    "            label.append(row[2])\n",
    "        return data_pre[1:], data_post[1:], label[1:] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data_mnli(path, filename):\n",
    "    with open(current_path+'/hw2_data/' + filename) as tsv:\n",
    "        reader = csv.reader(tsv, delimiter='\\t')\n",
    "        counter = 0\n",
    "        genre = []\n",
    "        result = {}\n",
    "        for row in reader:\n",
    "            temp_list = []\n",
    "            if counter > 0:\n",
    "                genre = row[3]\n",
    "                if genre in result:\n",
    "                    temp_list.append(row[0])\n",
    "                    temp_list.append(row[1])\n",
    "                    temp_list.append(row[2])\n",
    "                    result[genre].append(temp_list)\n",
    "                else:\n",
    "                    result[genre] = []\n",
    "                    temp_list.append(row[0])\n",
    "                    temp_list.append(row[1])\n",
    "                    temp_list.append(row[2])\n",
    "                    result[genre].append(temp_list)\n",
    "            else:\n",
    "                counter += 1\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "snli_train_pre, snli_train_post, snli_train_label = read_data(current_path, 'snli_train.tsv') \n",
    "snli_val_pre, snli_val_post, snli_val_label = read_data(current_path, 'snli_val.tsv') \n",
    "mnli_train = read_data_mnli(current_path, 'mnli_train.tsv') \n",
    "mnli_val= read_data_mnli(current_path, 'mnli_val.tsv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load embedding layers\n",
    "def load_vectors(fname, vocab_size):\n",
    "    fin = io.open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "    n, d = map(int, fin.readline().split())\n",
    "    data = {}\n",
    "    counter = 0\n",
    "    for line in fin:\n",
    "        tokens = line.rstrip().split(' ')\n",
    "        temp_list = []\n",
    "        for item in tokens[1:]:\n",
    "            temp_list.append(float(item))\n",
    "        data[tokens[0]] = temp_list\n",
    "        if counter % 1000 == 0:\n",
    "            print('Finished line {}'.format(counter))\n",
    "        counter += 1\n",
    "        if counter >= vocab_size:\n",
    "            break;\n",
    "    return d, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished line 0\n",
      "Finished line 1000\n",
      "Finished line 2000\n",
      "Finished line 3000\n",
      "Finished line 4000\n",
      "Finished line 5000\n",
      "Finished line 6000\n",
      "Finished line 7000\n",
      "Finished line 8000\n",
      "Finished line 9000\n",
      "Finished line 10000\n",
      "Finished line 11000\n",
      "Finished line 12000\n",
      "Finished line 13000\n",
      "Finished line 14000\n",
      "Finished line 15000\n",
      "Finished line 16000\n",
      "Finished line 17000\n",
      "Finished line 18000\n",
      "Finished line 19000\n",
      "Finished line 20000\n",
      "Finished line 21000\n",
      "Finished line 22000\n",
      "Finished line 23000\n",
      "Finished line 24000\n",
      "Finished line 25000\n",
      "Finished line 26000\n",
      "Finished line 27000\n",
      "Finished line 28000\n",
      "Finished line 29000\n",
      "Finished line 30000\n",
      "Finished line 31000\n",
      "Finished line 32000\n",
      "Finished line 33000\n",
      "Finished line 34000\n",
      "Finished line 35000\n",
      "Finished line 36000\n",
      "Finished line 37000\n",
      "Finished line 38000\n",
      "Finished line 39000\n",
      "Finished line 40000\n",
      "Finished line 41000\n",
      "Finished line 42000\n",
      "Finished line 43000\n",
      "Finished line 44000\n",
      "Finished line 45000\n",
      "Finished line 46000\n",
      "Finished line 47000\n",
      "Finished line 48000\n",
      "Finished line 49000\n"
     ]
    }
   ],
   "source": [
    "embed_dim, embed_dist = load_vectors(current_path+'/hw2_data/wiki-news-300d-1M.vec', 50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create label set\n",
    "def convert_label(label_list):\n",
    "    result = []\n",
    "    for item in label_list:\n",
    "        if item == 'contradiction':\n",
    "            result.append(0)\n",
    "        elif item == 'entailment':\n",
    "            result.append(1)\n",
    "        elif item == 'neutral':\n",
    "            result.append(2)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "snli_train_label = convert_label(snli_train_label)\n",
    "snli_val_label = convert_label(snli_val_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_token(input_set):\n",
    "    result = []\n",
    "    counter = 0\n",
    "    for row in input_set:\n",
    "        result.append(row.split(' '))\n",
    "        if counter % 5000 == 0:\n",
    "            print('FINISHED {}'.format(counter))\n",
    "        counter += 1\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINISHED 0\n",
      "FINISHED 5000\n",
      "FINISHED 10000\n",
      "FINISHED 15000\n",
      "FINISHED 20000\n",
      "FINISHED 25000\n",
      "FINISHED 30000\n",
      "FINISHED 35000\n",
      "FINISHED 40000\n",
      "FINISHED 45000\n",
      "FINISHED 50000\n",
      "FINISHED 55000\n",
      "FINISHED 60000\n",
      "FINISHED 65000\n",
      "FINISHED 70000\n",
      "FINISHED 75000\n",
      "FINISHED 80000\n",
      "FINISHED 85000\n",
      "FINISHED 90000\n",
      "FINISHED 95000\n",
      "FINISHED 0\n",
      "FINISHED 5000\n",
      "FINISHED 10000\n",
      "FINISHED 15000\n",
      "FINISHED 20000\n",
      "FINISHED 25000\n",
      "FINISHED 30000\n",
      "FINISHED 35000\n",
      "FINISHED 40000\n",
      "FINISHED 45000\n",
      "FINISHED 50000\n",
      "FINISHED 55000\n",
      "FINISHED 60000\n",
      "FINISHED 65000\n",
      "FINISHED 70000\n",
      "FINISHED 75000\n",
      "FINISHED 80000\n",
      "FINISHED 85000\n",
      "FINISHED 90000\n",
      "FINISHED 95000\n",
      "FINISHED 0\n",
      "FINISHED 0\n"
     ]
    }
   ],
   "source": [
    "snli_train_pre = convert_to_token(snli_train_pre)\n",
    "snli_train_post = convert_to_token(snli_train_post)\n",
    "\n",
    "snli_val_pre = convert_to_token(snli_val_pre) \n",
    "snli_val_post = convert_to_token(snli_val_post)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all_tokens(input_set):\n",
    "    all_sentence = []\n",
    "    for sentence in input_set:\n",
    "        all_sentence += sentence\n",
    "    output = Counter(all_sentence)\n",
    "    result = []\n",
    "    for key, value in output.items():\n",
    "        result.append(key)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens = find_all_tokens(snli_train_pre+snli_train_post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lookup(all_tokens, embed_dist):\n",
    "    result = {}\n",
    "    counter = 1\n",
    "    for item in all_tokens:\n",
    "        if item in embed_dist:\n",
    "            result[item] = counter\n",
    "        else:\n",
    "            result[item] = 0\n",
    "        counter += 1\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup = build_lookup(all_tokens, embed_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rebuild_embed_dict(embed_dict, lookup, embed_dim):\n",
    "    result = {}\n",
    "    result[0] = [0 for i in range(embed_dim)]\n",
    "    for key, value in lookup.items():\n",
    "        if key in embed_dict:\n",
    "            result[value] = embed_dict[key]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_embed_dict = rebuild_embed_dict(embed_dist, lookup, embed_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_idx(input_set, lookup):\n",
    "    result = []\n",
    "    for sentence in input_set:\n",
    "        temp_result = []\n",
    "        for token in sentence:\n",
    "            if token in lookup:\n",
    "                temp_result.append(lookup[token])\n",
    "        result.append(temp_result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "snli_train_pre = convert_to_idx(snli_train_pre, lookup)\n",
    "snli_train_post = convert_to_idx(snli_train_post, lookup)\n",
    "\n",
    "snli_val_pre = convert_to_idx(snli_val_pre, lookup)\n",
    "snli_val_post = convert_to_idx(snli_val_post, lookup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_max_length(list1, list2):\n",
    "    length1 = []\n",
    "    length2 = []\n",
    "    for item in list1:\n",
    "        if len(item) != 0:\n",
    "            length1.append(len(item))\n",
    "        else:\n",
    "            length1.append(1)\n",
    "    for item in list2:\n",
    "        if len(item) != 0:\n",
    "            length2.append(len(item))\n",
    "        else:\n",
    "            length2.append(1)\n",
    "    return max([max(length1), max(length2)]), length1, length2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_WORD_LENGTH, snli_train_pre_len, snli_train_post_len = find_max_length(snli_train_pre, snli_train_post)\n",
    "_, snli_val_pre_len, snli_val_post_len = find_max_length(snli_val_pre, snli_val_post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def secure_idx(input_set, max_length):\n",
    "    for sentence in input_set:\n",
    "        if len(sentence) == 0:\n",
    "            for i in range(max_length):\n",
    "                sentence.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "secure_idx(snli_train_pre, MAX_WORD_LENGTH)\n",
    "secure_idx(snli_train_post, MAX_WORD_LENGTH)\n",
    "secure_idx(snli_val_pre, MAX_WORD_LENGTH)\n",
    "secure_idx(snli_val_post, MAX_WORD_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_each_genre(data_set, embed_dist, embed_dim):\n",
    "    result = {}\n",
    "    for genre, value in data_set.items():\n",
    "        temp_list = []\n",
    "        pre = []\n",
    "        post = []\n",
    "        label = []\n",
    "        for sample in value:\n",
    "            pre.append(sample[0])\n",
    "            post.append(sample[1])\n",
    "            label.append(sample[2])\n",
    "        pre = convert_to_token(pre)\n",
    "        post = convert_to_token(post)\n",
    "        all_tokens = find_all_tokens(pre+post)\n",
    "        lookup = build_lookup(all_tokens, embed_dist)\n",
    "        idx_embed_dict = rebuild_embed_dict(embed_dist, lookup, embed_dim)\n",
    "        pre = convert_to_idx(pre, lookup)\n",
    "        post = convert_to_idx(post, lookup)\n",
    "        max_length, pre_lengths, post_lengths = find_max_length(pre, post)\n",
    "        secure_idx(pre, max_length)\n",
    "        secure_idx(post, max_length)\n",
    "        temp_list.append(pre)\n",
    "        temp_list.append(post)\n",
    "        temp_list.append(pre_lengths)\n",
    "        temp_list.append(post_lengths)\n",
    "        temp_list.append(convert_label(label))\n",
    "        result[genre] = temp_list\n",
    "    return result, idx_embed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINISHED 0\n",
      "FINISHED 0\n",
      "FINISHED 0\n",
      "FINISHED 0\n",
      "FINISHED 0\n",
      "FINISHED 0\n",
      "FINISHED 0\n",
      "FINISHED 0\n",
      "FINISHED 0\n",
      "FINISHED 0\n",
      "FINISHED 0\n",
      "FINISHED 0\n",
      "FINISHED 0\n",
      "FINISHED 0\n",
      "FINISHED 0\n",
      "FINISHED 0\n",
      "FINISHED 0\n",
      "FINISHED 0\n",
      "FINISHED 0\n",
      "FINISHED 0\n"
     ]
    }
   ],
   "source": [
    "mnli_train, _ = setup_each_genre(mnli_train, embed_dist, embed_dim)\n",
    "mnli_val, idx_embed_dict_mn = setup_each_genre(mnli_val, embed_dist, embed_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu')\n",
    "if torch.cuda.is_available and torch.has_cudnn:\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "class localDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_pre, data_post, pre_length, post_length, data_label):\n",
    "        \"\"\"\n",
    "        @param data_list: list of character\n",
    "        @param target_list: list of targets\n",
    "\n",
    "        \"\"\"\n",
    "        self.data_pre, self.data_post, self.data_label = data_pre, data_post, data_label\n",
    "        self.pre_length = pre_length\n",
    "        self.post_length = post_length\n",
    "        assert (len(self.data_pre) == len(self.data_post)) and (len(self.data_pre) == len(self.data_label))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_pre)\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        \"\"\"\n",
    "        idx_pre = self.data_pre[key]\n",
    "        idx_post = self.data_post[key]\n",
    "        pre_length = self.pre_length[key]\n",
    "        post_length = self.post_length[key]\n",
    "        idx_label = self.data_label[key]\n",
    "        return [idx_pre, idx_post, pre_length, post_length, idx_label]\n",
    "\n",
    "def vocab_collate_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all\n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    data_pre = []\n",
    "    data_post = []\n",
    "    label_list = []\n",
    "    length_pre = []\n",
    "    length_post = []\n",
    "\n",
    "    for datum in batch:\n",
    "        label_list.append(datum[4])\n",
    "        length_pre.append(datum[2])\n",
    "        length_post.append(datum[3])\n",
    "    # padding\n",
    "    for datum in batch:\n",
    "        padded_vec_pre = np.pad(np.array(datum[0]),\n",
    "                                pad_width=((0,MAX_WORD_LENGTH-datum[2])),\n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        data_pre.append(padded_vec_pre)\n",
    "        padded_vec_post = np.pad(np.array(datum[1]),\n",
    "                                pad_width=((0,MAX_WORD_LENGTH-datum[3])),\n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        data_post.append(padded_vec_post)\n",
    "    \n",
    "    pre_dec_order = np.argsort(length_pre)[::-1] # Sort all list accoridng to length in decreasing order\n",
    "    post_dec_order = np.argsort(length_post)[::-1]\n",
    "    data_pre = np.array(data_pre)[pre_dec_order]\n",
    "    data_post = np.array(data_post)[pre_dec_order]\n",
    "    length_pre = np.array(length_pre)[pre_dec_order]\n",
    "    length_post = np.array(length_post)[post_dec_order]\n",
    "    label_list = np.array(label_list)[pre_dec_order]\n",
    "    data_pre = torch.from_numpy(np.array(data_pre))\n",
    "    data_pre = data_pre.type('torch.FloatTensor')\n",
    "    data_post = torch.from_numpy(np.array(data_post))\n",
    "    data_post = data_post.type('torch.FloatTensor')\n",
    "    return [data_pre, data_post, torch.LongTensor(length_pre), torch.LongTensor(length_post), torch.LongTensor(label_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build train, valid and test dataloaders\n",
    "BATCH_SIZE =128\n",
    "\n",
    "snli_train_dataset = localDataset(snli_train_pre, snli_train_post, snli_train_pre_len, snli_train_post_len, snli_train_label)\n",
    "snli_train_loader = torch.utils.data.DataLoader(dataset=snli_train_dataset,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=vocab_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "snli_val_dataset = localDataset(snli_val_pre, snli_val_post, snli_val_pre_len, snli_val_post_len, snli_val_label)\n",
    "snli_val_loader = torch.utils.data.DataLoader(dataset=snli_val_dataset,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=vocab_collate_func,\n",
    "                                           shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU(nn.Module):\n",
    "    def __init__(self, emb_size, hidden_size, num_layers, num_classes):\n",
    "        # RNN Accepts the following hyperparams:\n",
    "        # emb_size: Embedding Size\n",
    "        # hidden_size: Hidden Size of layer in RNN\n",
    "        # num_layers: number of layers in RNN\n",
    "        # num_classes: number of output classes\n",
    "        # vocab_size: vocabulary size\n",
    "        super(GRU, self).__init__()\n",
    "\n",
    "        self.num_layers, self.hidden_size = num_layers, hidden_size\n",
    "        #self.embedding = nn.Embedding(vocab_size, emb_size, padding_idx=PAD_IDX)\n",
    "        self.rnn = nn.GRU(emb_size, hidden_size, num_layers, batch_first=True, bidirectional=True)\n",
    "        # Details can be seen at https://pytorch.org/docs/stable/_modules/torch/nn/modules/rnn.html \n",
    "        self.linear = nn.Linear(4*hidden_size, num_classes)\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        # Function initializes the activation of recurrent neural net at timestep 0\n",
    "        # Needs to be in format (num_layers, batch_size, hidden_size)\n",
    "        hidden = torch.randn(2*self.num_layers, batch_size, self.hidden_size)\n",
    "\n",
    "        return hidden.to(device)\n",
    "    \n",
    "    def perform_embed(self, data_set, idx_embed_dict):\n",
    "        result = []\n",
    "        for sentence in data_set.numpy().tolist():\n",
    "            temp_result = []\n",
    "            for idx in sentence:\n",
    "                temp_result.append(idx_embed_dict[idx])\n",
    "            result.append(temp_result)\n",
    "        return torch.from_numpy(np.array(result)).type('torch.FloatTensor').to(device)\n",
    "\n",
    "    def forward(self, data_pre, data_post, pre_lengths, post_lengths, idx_embed_dict):\n",
    "        # reset hidden state\n",
    "\n",
    "        batch_size,_ = data_pre.size()\n",
    "\n",
    "        self.hidden_pre = self.init_hidden(batch_size)\n",
    "        self.hidden_post = self.init_hidden(batch_size)        \n",
    "\n",
    "        # get embedding of characters\n",
    "        embed_pre = self.perform_embed(data_pre.cpu(), idx_embed_dict)#.type('torch.FloatTensor')\n",
    "        embed_post = self.perform_embed(data_post.cpu(), idx_embed_dict)#.type('torch.FloatTensor')\n",
    "        \n",
    "        # pack padded sequence\n",
    "        embed_pre = torch.nn.utils.rnn.pack_padded_sequence(embed_pre, pre_lengths.numpy(), batch_first=True)\n",
    "        embed_post = torch.nn.utils.rnn.pack_padded_sequence(embed_post, post_lengths.numpy(), batch_first=True)\n",
    "        # According to Pytorch, the pack_padded_sequance take input with dimension batch_size if batch_first == True\n",
    "        # and maximum sequence length and embedding dimensions refer to https://pytorch.org/docs/stable/nn.html \n",
    "        \n",
    "        # fprop though RNN\n",
    "        rnn_out_pre, self.hidden_pre = self.rnn(embed_pre, self.hidden_pre)\n",
    "        rnn_out_post, self.hidden_post = self.rnn(embed_post, self.hidden_post)\n",
    "        \n",
    "        # undo packing\n",
    "        rnn_out_pre, _ = torch.nn.utils.rnn.pad_packed_sequence(rnn_out_pre, batch_first=True)\n",
    "        rnn_out_post, _ = torch.nn.utils.rnn.pad_packed_sequence(rnn_out_post, batch_first=True)\n",
    "        \n",
    "        # sum hidden activations of RNN across time\n",
    "        rnn_out_pre = torch.sum(rnn_out_pre, dim=1)\n",
    "        rnn_out_post = torch.sum(rnn_out_post, dim=1)\n",
    "        rnn_out = torch.cat((rnn_out_pre, rnn_out_post), 1)\n",
    "\n",
    "        logits = self.linear(rnn_out)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/10], Step: [101/782], Validation Acc: 36.2\n",
      "Epoch: [1/10], Step: [201/782], Validation Acc: 39.1\n",
      "Epoch: [1/10], Step: [301/782], Validation Acc: 43.1\n",
      "Epoch: [1/10], Step: [401/782], Validation Acc: 45.8\n",
      "Epoch: [1/10], Step: [501/782], Validation Acc: 48.0\n",
      "Epoch: [1/10], Step: [601/782], Validation Acc: 46.2\n",
      "Epoch: [1/10], Step: [701/782], Validation Acc: 47.4\n",
      "Epoch: [2/10], Step: [101/782], Validation Acc: 51.4\n",
      "Epoch: [2/10], Step: [201/782], Validation Acc: 52.1\n",
      "Epoch: [2/10], Step: [301/782], Validation Acc: 50.6\n",
      "Epoch: [2/10], Step: [401/782], Validation Acc: 50.8\n",
      "Epoch: [2/10], Step: [501/782], Validation Acc: 51.8\n",
      "Epoch: [2/10], Step: [601/782], Validation Acc: 52.8\n",
      "Epoch: [2/10], Step: [701/782], Validation Acc: 53.4\n",
      "Epoch: [3/10], Step: [101/782], Validation Acc: 54.3\n",
      "Epoch: [3/10], Step: [201/782], Validation Acc: 53.3\n",
      "Epoch: [3/10], Step: [301/782], Validation Acc: 52.6\n",
      "Epoch: [3/10], Step: [401/782], Validation Acc: 53.3\n",
      "Epoch: [3/10], Step: [501/782], Validation Acc: 53.6\n",
      "Epoch: [3/10], Step: [601/782], Validation Acc: 53.6\n",
      "Epoch: [3/10], Step: [701/782], Validation Acc: 55.2\n",
      "Epoch: [4/10], Step: [101/782], Validation Acc: 55.0\n",
      "Epoch: [4/10], Step: [201/782], Validation Acc: 54.1\n",
      "Epoch: [4/10], Step: [301/782], Validation Acc: 54.8\n",
      "Epoch: [4/10], Step: [401/782], Validation Acc: 55.7\n",
      "Epoch: [4/10], Step: [501/782], Validation Acc: 53.5\n",
      "Epoch: [4/10], Step: [601/782], Validation Acc: 54.5\n",
      "Epoch: [4/10], Step: [701/782], Validation Acc: 56.5\n",
      "Epoch: [5/10], Step: [101/782], Validation Acc: 55.5\n",
      "Epoch: [5/10], Step: [201/782], Validation Acc: 55.8\n",
      "Epoch: [5/10], Step: [301/782], Validation Acc: 55.4\n",
      "Epoch: [5/10], Step: [401/782], Validation Acc: 54.5\n",
      "Epoch: [5/10], Step: [501/782], Validation Acc: 55.2\n",
      "Epoch: [5/10], Step: [601/782], Validation Acc: 56.7\n",
      "Epoch: [5/10], Step: [701/782], Validation Acc: 56.0\n",
      "Epoch: [6/10], Step: [101/782], Validation Acc: 56.2\n",
      "Epoch: [6/10], Step: [201/782], Validation Acc: 54.7\n",
      "Epoch: [6/10], Step: [301/782], Validation Acc: 56.8\n",
      "Epoch: [6/10], Step: [401/782], Validation Acc: 56.1\n",
      "Epoch: [6/10], Step: [501/782], Validation Acc: 55.9\n",
      "Epoch: [6/10], Step: [601/782], Validation Acc: 55.1\n",
      "Epoch: [6/10], Step: [701/782], Validation Acc: 57.7\n",
      "Epoch: [7/10], Step: [101/782], Validation Acc: 53.7\n",
      "Epoch: [7/10], Step: [201/782], Validation Acc: 56.7\n",
      "Epoch: [7/10], Step: [301/782], Validation Acc: 55.7\n",
      "Epoch: [7/10], Step: [401/782], Validation Acc: 56.2\n",
      "Epoch: [7/10], Step: [501/782], Validation Acc: 54.2\n",
      "Epoch: [7/10], Step: [601/782], Validation Acc: 56.8\n",
      "Epoch: [7/10], Step: [701/782], Validation Acc: 57.1\n",
      "Epoch: [8/10], Step: [101/782], Validation Acc: 56.0\n",
      "Epoch: [8/10], Step: [201/782], Validation Acc: 57.3\n",
      "Epoch: [8/10], Step: [301/782], Validation Acc: 56.7\n",
      "Epoch: [8/10], Step: [401/782], Validation Acc: 57.5\n",
      "Epoch: [8/10], Step: [501/782], Validation Acc: 55.2\n",
      "Epoch: [8/10], Step: [601/782], Validation Acc: 56.2\n",
      "Epoch: [8/10], Step: [701/782], Validation Acc: 57.6\n",
      "Epoch: [9/10], Step: [101/782], Validation Acc: 54.7\n",
      "Epoch: [9/10], Step: [201/782], Validation Acc: 56.7\n",
      "Epoch: [9/10], Step: [301/782], Validation Acc: 55.9\n",
      "Epoch: [9/10], Step: [401/782], Validation Acc: 56.3\n",
      "Epoch: [9/10], Step: [501/782], Validation Acc: 55.3\n",
      "Epoch: [9/10], Step: [601/782], Validation Acc: 58.7\n",
      "Epoch: [9/10], Step: [701/782], Validation Acc: 58.4\n",
      "Epoch: [10/10], Step: [101/782], Validation Acc: 56.9\n",
      "Epoch: [10/10], Step: [201/782], Validation Acc: 57.9\n",
      "Epoch: [10/10], Step: [301/782], Validation Acc: 55.7\n",
      "Epoch: [10/10], Step: [401/782], Validation Acc: 58.5\n",
      "Epoch: [10/10], Step: [501/782], Validation Acc: 57.7\n",
      "Epoch: [10/10], Step: [601/782], Validation Acc: 57.2\n",
      "Epoch: [10/10], Step: [701/782], Validation Acc: 57.9\n"
     ]
    }
   ],
   "source": [
    "def test_model(loader, model, idx_embed_dict):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    for data_pre, data_post, lengths_pre, lengths_post, labels in loader:\n",
    "        data_batch_pre, lengths_batch_pre, data_batch_post, lengths_batch_post, label_batch = data_pre.to(device), lengths_pre, data_post.to(device), lengths_post, labels\n",
    "        outputs = F.softmax(model(data_batch_pre, data_batch_post, lengths_batch_pre, lengths_batch_post, idx_embed_dict), dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels.to(device).view_as(predicted)).sum().item()\n",
    "    return (100 * correct / total)\n",
    "\n",
    "\n",
    "model = GRU(emb_size=embed_dim, hidden_size=200, num_layers=2, num_classes=3).cuda()\n",
    "\n",
    "learning_rate = 3e-4\n",
    "num_epochs = 10 # number epoch to train\n",
    "\n",
    "# Criterion and Optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "total_step = len(snli_val_loader)\n",
    "#print(len(snli_train_loader))\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (data_pre, data_post, pre_lengths, post_lengths, labels) in enumerate(snli_train_loader):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        labels.to(device)\n",
    "        outputs = model(data_pre, data_post, pre_lengths, post_lengths, idx_embed_dict)\n",
    "        loss = criterion(outputs, labels.to(device))\n",
    "\n",
    "        # Backward and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # validate every 100 iterations\n",
    "        if i > 0 and i % 100 == 0:\n",
    "            # validate\n",
    "            val_acc = test_model(snli_val_loader, model, idx_embed_dict)\n",
    "            print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}'.format(\n",
    "                       epoch+1, num_epochs, i+1, len(snli_train_loader), val_acc))\n",
    "final_accuracy = test_model(snli_val_loader, model, idx_embed_dict)\n",
    "print('Final Test Acc: {}'.format(final_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN_list = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "class localDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_pre, data_post, pre_length, post_length, data_label):\n",
    "        \"\"\"\n",
    "        @param data_list: list of character\n",
    "        @param target_list: list of targets\n",
    "\n",
    "        \"\"\"\n",
    "        self.data_pre, self.data_post, self.data_label = data_pre, data_post, data_label\n",
    "        self.pre_length = pre_length\n",
    "        self.post_length = post_length\n",
    "        assert (len(self.data_pre) == len(self.data_post)) and (len(self.data_pre) == len(self.data_label))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_pre)\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        \"\"\"\n",
    "        idx_pre = self.data_pre[key]\n",
    "        idx_post = self.data_post[key]\n",
    "        pre_length = self.pre_length[key]\n",
    "        post_length = self.post_length[key]\n",
    "        idx_label = self.data_label[key]\n",
    "        return [idx_pre, idx_post, pre_length, post_length, idx_label]\n",
    "\n",
    "def vocab_collate_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all\n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    data_pre = []\n",
    "    data_post = []\n",
    "    label_list = []\n",
    "    length_pre = []\n",
    "    length_post = []\n",
    "\n",
    "    for datum in batch:\n",
    "        label_list.append(datum[4])\n",
    "        length_pre.append(datum[2])\n",
    "        length_post.append(datum[3])\n",
    "    # padding\n",
    "    for datum in batch:\n",
    "        padded_vec_pre = np.pad(np.array(datum[0]),\n",
    "                                pad_width=((0,MAX_WORD_LENGTH-datum[2])),\n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        data_pre.append(padded_vec_pre)\n",
    "        padded_vec_post = np.pad(np.array(datum[1]),\n",
    "                                pad_width=((0,MAX_WORD_LENGTH-datum[3])),\n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        data_post.append(padded_vec_post)\n",
    "    \n",
    "    pre_dec_order = np.argsort(length_pre)[::-1] # Sort all list accoridng to length in decreasing order\n",
    "    post_dec_order = np.argsort(length_post)[::-1]\n",
    "    data_pre = np.array(data_pre)[pre_dec_order]\n",
    "    data_post = np.array(data_post)[pre_dec_order]\n",
    "    length_pre = np.array(length_pre)[pre_dec_order]\n",
    "    length_post = np.array(length_post)[post_dec_order]\n",
    "    label_list = np.array(label_list)[pre_dec_order]\n",
    "    data_pre = torch.from_numpy(np.array(data_pre)).type('torch.DoubleTensor')\n",
    "    data_post = torch.from_numpy(np.array(data_post)).type('torch.DoubleTensor')\n",
    "    return [data_pre, data_post, torch.LongTensor(length_pre), torch.LongTensor(length_post), torch.LongTensor(label_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build train, valid and test dataloaders\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "snli_train_dataset = localDataset(snli_train_pre, snli_train_post, snli_train_pre_len, snli_train_post_len, snli_train_label)\n",
    "snli_train_loader = torch.utils.data.DataLoader(dataset=snli_train_dataset,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=vocab_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "snli_val_dataset = localDataset(snli_val_pre, snli_val_post, snli_val_pre_len, snli_val_post_len, snli_val_label)\n",
    "snli_val_loader = torch.utils.data.DataLoader(dataset=snli_val_dataset,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=vocab_collate_func,\n",
    "                                           shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, emb_size, hidden_size, num_layers, num_classes, max_length):\n",
    "\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        self.num_layers, self.hidden_size = num_layers, hidden_size\n",
    "        \n",
    "        self.conv1 = nn.Conv1d(emb_size, hidden_size, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(hidden_size, hidden_size, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.maxpool = nn.MaxPool1d(max_length, stride=1)\n",
    "\n",
    "        self.linear = nn.Linear(2*hidden_size, hidden_size)\n",
    "        \n",
    "        self.linear2 = nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "    def perform_embed(self, data_set, idx_embed_dict):\n",
    "        result = []\n",
    "        for sentence in data_set.numpy().tolist():\n",
    "            temp_result = []\n",
    "            for idx in sentence:\n",
    "                temp_result.append(idx_embed_dict[idx])\n",
    "            result.append(temp_result)\n",
    "        return torch.from_numpy(np.array(result)).type('torch.FloatTensor').to(device)\n",
    "\n",
    "    def forward(self, data_pre, data_post, idx_embed_dict):\n",
    "        batch_size, max_lengths = data_pre.size()\n",
    "\n",
    "        data_pre = self.perform_embed(data_pre.cpu(), idx_embed_dict)#.type('torch.FloatTensor')\n",
    "        data_post = self.perform_embed(data_post.cpu(), idx_embed_dict)#.type('torch.FloatTensor')\n",
    "        #print('data_pre {}'.format(data_pre.is_cuda))\n",
    "        \n",
    "        hidden_pre = self.conv1(data_pre.transpose(1,2)).transpose(1,2)\n",
    "        hidden_pre = F.relu(hidden_pre.contiguous().view(-1, hidden_pre.size(-1))).view(batch_size, max_lengths, hidden_pre.size(-1))\n",
    "        \n",
    "        hidden_pre = self.conv2(hidden_pre.transpose(1,2)).transpose(1,2)\n",
    "        hidden_pre = F.relu(hidden_pre.contiguous().view(-1, hidden_pre.size(-1))).view(batch_size, max_lengths, hidden_pre.size(-1))\n",
    "        \n",
    "        hidden_post = self.conv1(data_post.transpose(1,2)).transpose(1,2)\n",
    "        hidden_post = F.relu(hidden_post.contiguous().view(-1, hidden_post.size(-1))).view(batch_size, max_lengths, hidden_post.size(-1))\n",
    "        \n",
    "        hidden_post = self.conv2(hidden_post.transpose(1,2)).transpose(1,2)\n",
    "        hidden_post = F.relu(hidden_post.contiguous().view(-1, hidden_post.size(-1))).view(batch_size, max_lengths, hidden_post.size(-1))\n",
    "        \n",
    "        hidden_pre = hidden_pre.transpose(1,2)\n",
    "        hidden_pre = self.maxpool(hidden_pre)\n",
    "        hidden_pre = hidden_pre.transpose(1,2).squeeze(1)\n",
    "        \n",
    "        hidden_post = hidden_post.transpose(1,2)\n",
    "        hidden_post = self.maxpool(hidden_post)\n",
    "        hidden_post = hidden_post.transpose(1,2).squeeze(1)\n",
    "        \n",
    "        hidden = torch.cat((hidden_pre, hidden_post), 1)\n",
    "        hidden = self.linear(hidden)\n",
    "        hidden = torch.relu(hidden)\n",
    "        logits = self.linear2(hidden)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/5], Step: [101/782], Validation Acc: 56.0\n",
      "Epoch: [1/5], Step: [201/782], Validation Acc: 61.5\n",
      "Epoch: [1/5], Step: [301/782], Validation Acc: 61.6\n",
      "Epoch: [1/5], Step: [401/782], Validation Acc: 63.5\n",
      "Epoch: [1/5], Step: [501/782], Validation Acc: 62.7\n",
      "Epoch: [1/5], Step: [601/782], Validation Acc: 62.7\n",
      "Epoch: [1/5], Step: [701/782], Validation Acc: 64.7\n",
      "Epoch: [2/5], Step: [101/782], Validation Acc: 65.1\n",
      "Epoch: [2/5], Step: [201/782], Validation Acc: 66.1\n",
      "Epoch: [2/5], Step: [301/782], Validation Acc: 66.7\n",
      "Epoch: [2/5], Step: [401/782], Validation Acc: 65.4\n",
      "Epoch: [2/5], Step: [501/782], Validation Acc: 68.0\n",
      "Epoch: [2/5], Step: [601/782], Validation Acc: 67.2\n",
      "Epoch: [2/5], Step: [701/782], Validation Acc: 67.8\n",
      "Epoch: [3/5], Step: [101/782], Validation Acc: 66.0\n",
      "Epoch: [3/5], Step: [201/782], Validation Acc: 67.3\n",
      "Epoch: [3/5], Step: [301/782], Validation Acc: 67.2\n",
      "Epoch: [3/5], Step: [401/782], Validation Acc: 69.6\n",
      "Epoch: [3/5], Step: [501/782], Validation Acc: 68.4\n",
      "Epoch: [3/5], Step: [601/782], Validation Acc: 69.1\n",
      "Epoch: [3/5], Step: [701/782], Validation Acc: 68.3\n",
      "Epoch: [4/5], Step: [101/782], Validation Acc: 68.6\n",
      "Epoch: [4/5], Step: [201/782], Validation Acc: 66.5\n",
      "Epoch: [4/5], Step: [301/782], Validation Acc: 67.8\n",
      "Epoch: [4/5], Step: [401/782], Validation Acc: 68.3\n",
      "Epoch: [4/5], Step: [501/782], Validation Acc: 68.6\n",
      "Epoch: [4/5], Step: [601/782], Validation Acc: 67.9\n",
      "Epoch: [4/5], Step: [701/782], Validation Acc: 68.8\n",
      "Epoch: [5/5], Step: [101/782], Validation Acc: 67.4\n",
      "Epoch: [5/5], Step: [201/782], Validation Acc: 66.8\n",
      "Epoch: [5/5], Step: [301/782], Validation Acc: 68.1\n",
      "Epoch: [5/5], Step: [401/782], Validation Acc: 66.9\n",
      "Epoch: [5/5], Step: [501/782], Validation Acc: 67.0\n",
      "Epoch: [5/5], Step: [601/782], Validation Acc: 69.1\n",
      "Epoch: [5/5], Step: [701/782], Validation Acc: 67.1\n",
      "Final Test Acc: 69.1\n"
     ]
    }
   ],
   "source": [
    "def test_model(loader, model, idx_embed_dict):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    for data_pre, data_post, pre_lengths, post_lengths, labels in loader:\n",
    "        data_batch_pre, data_batch_post, lengths_batch_pre, lengths_batch_post, label_batch = data_pre, data_post, pre_lengths, post_lengths, labels\n",
    "        outputs = F.softmax(model(data_batch_pre, data_batch_post, idx_embed_dict), dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels.to(device).view_as(predicted)).sum().item()\n",
    "    return (100 * correct / total)\n",
    "\n",
    "model = CNN(emb_size=300, hidden_size=1000, num_layers=2, num_classes=3, max_length=MAX_WORD_LENGTH).to(device)\n",
    "\n",
    "learning_rate = 3e-4\n",
    "num_epochs = 5 # number epoch to train\n",
    "\n",
    "# Criterion and Optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "total_step = len(snli_val_loader)\n",
    "    \n",
    "learning_loss = []\n",
    "learning_accu = []\n",
    "training_example = []\n",
    "final_accuracy = 0\n",
    "record = 0\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (data_pre, data_post, pre_lengths, post_lengths, labels) in enumerate(snli_train_loader):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        record += len(list(data_pre))\n",
    "        outputs = model(data_pre.to(device), data_post.to(device), idx_embed_dict)\n",
    "        loss = criterion(outputs, labels.to(device))\n",
    "\n",
    "        # Backward and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # validate every 100 iterations\n",
    "        if i > 0 and i % 100 == 0:\n",
    "            # validate\n",
    "            val_acc = test_model(snli_val_loader, model, idx_embed_dict)\n",
    "            print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}'.format(\n",
    "                        epoch+1, num_epochs, i+1, len(snli_train_loader), val_acc))\n",
    "            learning_loss.append(criterion(outputs, labels.to(device)))\n",
    "            learning_accu.append(val_acc)\n",
    "            training_example.append(record)\n",
    "final_accuracy = test_model(snli_val_loader, model, idx_embed_dict)\n",
    "print('Final Test Acc: {}'.format(final_accuracy))\n",
    "\n",
    "hidden_size = 300 # Change this variable when testing different parameters\n",
    "kernel_size = 3 # Change this variable when testing different parameters\n",
    "CNN_list[final_accuracy] = [hidden_size, kernel_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[12928,\n",
       " 25728,\n",
       " 38528,\n",
       " 51328,\n",
       " 64128,\n",
       " 76928,\n",
       " 89728,\n",
       " 112928,\n",
       " 125728,\n",
       " 138528,\n",
       " 151328,\n",
       " 164128,\n",
       " 176928,\n",
       " 189728,\n",
       " 212928,\n",
       " 225728,\n",
       " 238528,\n",
       " 251328,\n",
       " 264128,\n",
       " 276928,\n",
       " 289728,\n",
       " 312928,\n",
       " 325728,\n",
       " 338528,\n",
       " 351328,\n",
       " 364128,\n",
       " 376928,\n",
       " 389728,\n",
       " 412928,\n",
       " 425728,\n",
       " 438528,\n",
       " 451328,\n",
       " 464128,\n",
       " 476928,\n",
       " 489728]"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find Three correctly labeled samples \n",
    "def list_three_correct(loader, model, idx_embed):\n",
    "    model.eval()\n",
    "    correct_sample = []\n",
    "    correct_label = []\n",
    "    counter_c = 0\n",
    "    \n",
    "    for data_pre, data_post, pre_lengths, post_lengths, labels in loader:\n",
    "        data_batch_pre, data_batch_post, pre_length_batch, post_length_batch, label_batch = data_pre, data_post, pre_lengths, post_lengths, labels\n",
    "        outputs = F.softmax(model(data_pre, data_post, idx_embed), dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "        correction = list(predicted.eq(labels.view_as(predicted).to(device)))\n",
    "        label = list(labels)\n",
    "        data_content_pre = list(data_pre)\n",
    "        data_content_post = list(data_post)\n",
    "        inner_counter = 0\n",
    "        while inner_counter < len(correction):\n",
    "            if correction[inner_counter] == 1:\n",
    "                temp_list = []\n",
    "                temp_list.append(list(data_content_pre[inner_counter].numpy()))\n",
    "                temp_list.append(list(data_content_post[inner_counter].numpy()))\n",
    "                correct_sample.append(temp_list)\n",
    "                correct_label.append(label[inner_counter].item())\n",
    "                #(data_content[inner_counter])\n",
    "                inner_counter += 1\n",
    "                counter_c += 1\n",
    "                if counter_c == 3:\n",
    "                    break\n",
    "            else:\n",
    "                inner_counter += 1\n",
    "        break\n",
    "    \n",
    "    return correct_sample, correct_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_sample, correct_label = list_three_correct(snli_val_loader, model, idx_embed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "for item in correct_sample:\n",
    "    pre = item[0]\n",
    "    post = item[1]\n",
    "    pre_result = ''\n",
    "    post_result = ''\n",
    "    for inneritem in pre:\n",
    "        for key, value in lookup.items():\n",
    "            if value != 0  and value == inneritem:\n",
    "                pre_result += key + ' '\n",
    "                break\n",
    "    for inneritem in post:\n",
    "        for key, value in lookup.items():\n",
    "            if value != 0  and value == inneritem:\n",
    "                post_result += key + ' '\n",
    "                break\n",
    "    result.append([pre_result, post_result])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab_correct = []\n",
    "for item in correct_label:\n",
    "    if item == 0:\n",
    "        lab_correct.append('contradiction')\n",
    "    elif item == 1:\n",
    "        lab_correct.append('entailment')\n",
    "    else:\n",
    "        lab_correct.append('neutral')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "fincorrect = open('three_correct_reviews.txt','w')\n",
    "for i in range(3):\n",
    "    fincorrect.write('The premice: ' + str(result[i][0]) + ' The hypothesis: ' + str(result[i][1]) + ' is correctly labeled as ' + str(lab_correct[i]) + '\\n')\n",
    "fincorrect.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find Three correctly labeled samples \n",
    "def list_three_incorrect(loader, model, idx_embed):\n",
    "    model.eval()\n",
    "    correct_sample = []\n",
    "    correct_label = []\n",
    "    counter_c = 0\n",
    "    \n",
    "    for data_pre, data_post, pre_lengths, post_lengths, labels in loader:\n",
    "        data_batch_pre, data_batch_post, pre_length_batch, post_length_batch, label_batch = data_pre, data_post, pre_lengths, post_lengths, labels\n",
    "        outputs = F.softmax(model(data_pre, data_post, idx_embed), dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "        correction = list(predicted.eq(labels.view_as(predicted).to(device)))\n",
    "        label = list(labels)\n",
    "        data_content_pre = list(data_pre)\n",
    "        data_content_post = list(data_post)\n",
    "        inner_counter = 0\n",
    "        while inner_counter < len(correction):\n",
    "            if correction[inner_counter] == 0:\n",
    "                temp_list = []\n",
    "                temp_list.append(list(data_content_pre[inner_counter].numpy()))\n",
    "                temp_list.append(list(data_content_post[inner_counter].numpy()))\n",
    "                correct_sample.append(temp_list)\n",
    "                correct_label.append(label[inner_counter].item())\n",
    "                #(data_content[inner_counter])\n",
    "                inner_counter += 1\n",
    "                counter_c += 1\n",
    "                if counter_c == 3:\n",
    "                    break\n",
    "            else:\n",
    "                inner_counter += 1\n",
    "        break\n",
    "    \n",
    "    return correct_sample, correct_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "incorrect_sample, incorrect_label = list_three_incorrect(snli_val_loader, model, idx_embed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "for item in incorrect_sample:\n",
    "    pre = item[0]\n",
    "    post = item[1]\n",
    "    pre_result = ''\n",
    "    post_result = ''\n",
    "    for inneritem in pre:\n",
    "        for key, value in lookup.items():\n",
    "            if value != 0  and value == inneritem:\n",
    "                pre_result += key + ' '\n",
    "                break\n",
    "    for inneritem in post:\n",
    "        for key, value in lookup.items():\n",
    "            if value != 0  and value == inneritem:\n",
    "                post_result += key + ' '\n",
    "                break\n",
    "    result.append([pre_result, post_result])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab_correct = []\n",
    "for item in incorrect_label:\n",
    "    if item == 0:\n",
    "        lab_correct.append('contradiction')\n",
    "    elif item == 1:\n",
    "        lab_correct.append('entailment')\n",
    "    else:\n",
    "        lab_correct.append('neutral')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "fincorrect = open('three_incorrect_reviews.txt','w')\n",
    "for i in range(3):\n",
    "    fincorrect.write('The premice: ' + str(result[i][0]) + ' The hypothesis: ' + str(result[i][1]) + ' is incorrectly labeled as ' + str(lab_correct[i]) + '\\n')\n",
    "fincorrect.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mnli_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "1861.0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-371-4c7f0d5bce85>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;31m# Forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_pre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_post\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx_embed_dict_mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-300-9ebcb2008bcf>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, data_pre, data_post, idx_embed_dict)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_lengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_pre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mdata_pre\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperform_embed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_pre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx_embed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#.type('torch.FloatTensor')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0mdata_post\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperform_embed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_post\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx_embed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#.type('torch.FloatTensor')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;31m#print('data_pre {}'.format(data_pre.is_cuda))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-300-9ebcb2008bcf>\u001b[0m in \u001b[0;36mperform_embed\u001b[0;34m(self, data_set, idx_embed_dict)\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mtemp_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m                 \u001b[0mtemp_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx_embed_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m             \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'torch.FloatTensor'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 1861.0"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 128\n",
    "\n",
    "def test_model(loader, model, idx_embed_dict):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    for data_pre, data_post, pre_lengths, post_lengths, labels in loader:\n",
    "        data_batch_pre, data_batch_post, lengths_batch_pre, lengths_batch_post, label_batch = data_pre, data_post, pre_lengths, post_lengths, labels\n",
    "        outputs = F.softmax(model(data_batch_pre, data_batch_post, idx_embed_dict), dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels.to(device).view_as(predicted)).sum().item()\n",
    "    return (100 * correct / total)\n",
    "\n",
    "result = {}\n",
    "for genre, data_set in mnli_val.items():\n",
    "    genre_current = genre\n",
    "    temp_result = []\n",
    "    MAX_WORD_LENGTH,_,_ = find_max_length(data_set[0], data_set[1])\n",
    "    mnli_val_dataset = localDataset(data_set[0], data_set[1], data_set[2], data_set[3], data_set[4])\n",
    "    mnli_val_loader = torch.utils.data.DataLoader(dataset=mnli_val_dataset,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=vocab_collate_func,\n",
    "                                           shuffle=True)\n",
    "    model = CNN(emb_size=300, hidden_size=100, num_layers=2, num_classes=3, max_length=MAX_WORD_LENGTH)\n",
    "\n",
    "    learning_rate = 3e-4\n",
    "    num_epochs = 5 # number epoch to train\n",
    "\n",
    "    # Criterion and Optimizer\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Train the model\n",
    "    total_step = len(snli_val_loader)\n",
    "    learning_loss = []\n",
    "    learning_accu = []\n",
    "    training_example = []\n",
    "    final_accuracy = 0\n",
    "    record = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (data_pre, data_post, pre_lengths, post_lengths, labels) in enumerate(mnli_val_loader):\n",
    "            record += len(list(data_pre))\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass\n",
    "            outputs = model(data_pre.to(device), data_post.to(device), idx_embed_dict_mn)\n",
    "            loss = criterion(outputs, labels.to(device))\n",
    "\n",
    "            # Backward and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # validate every 100 iterations\n",
    "            if i > 0 and i % 20 == 0:\n",
    "                # validate\n",
    "                val_acc = test_model(mnli_val_loader, model, idx_embed_dict)\n",
    "                print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}'.format(\n",
    "                           epoch+1, num_epochs, i+1, len(snli_train_loader), val_acc))\n",
    "                learning_loss.append(criterion(outputs, labels))\n",
    "                learning_accu.append(val_acc)\n",
    "                training_example.append(record)\n",
    "                final_accuracy = test_model(test_loader, model)\n",
    "                print('Final Test Acc: {}'.format(final_accuracy))\n",
    "    temp_result.append(final_accuracy)\n",
    "    temp_result.append(training_example)\n",
    "    temp_result.append(learning_accu)\n",
    "    result[genre] = temp_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,'Learning Curve')"
      ]
     },
     "execution_count": 351,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "x = np.asarray(training_example)\n",
    "y = np.asarray(learning_accu)\n",
    "plt.plot(x, y)\n",
    "\n",
    "#plt.xlim(5, 0)  # decreasing time\n",
    "\n",
    "plt.xlabel('learning examples')\n",
    "plt.ylabel('accuracy')\n",
    "plt.title('Learning Curve')\n",
    "#plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
